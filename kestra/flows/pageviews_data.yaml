id: pageviews-data
namespace: wikimedia-pageviews
description: Finds the latest pageviews URL

variables:
  base_url: "https://dumps.wikimedia.org/other/pageviews/"
  gcs_bucket_name: "wikimedia-pageviews-bucket"
  data_age: 3 # Files to be extracted, 1 file represents 1 hour of data

tasks:
  - id: latest_pageviews
    type: io.kestra.plugin.scripts.python.Script
    beforeCommands:
      - pip install requests beautifulsoup4
    script: |
      import requests
      from bs4 import BeautifulSoup
      import sys
      import json
      import os
      from kestra import Kestra


      def get_latest_pageviews_url(base_url: str):
          resp = requests.get(base_url)
          soup = BeautifulSoup(resp.content, 'html.parser')

          # Get latest year
          years = sorted([a['href'] for a in soup.find_all('a') if a['href'].strip('/').isdigit()], reverse=True)
          latest_year = years[0]

          # Get latest month
          resp_month = requests.get(f'{base_url}{latest_year}')
          soup_month = BeautifulSoup(resp_month.content, 'html.parser')
          months = sorted([a['href'] for a in soup_month.find_all('a') if \
                          latest_year.strip('/') in a['href']], 
                          reverse=True)
          latest_month = months[0]

          # Get latest file
          resp_file = requests.get(f'{base_url}{latest_year}{latest_month}')
          soup_file = BeautifulSoup(resp_file.content, 'html.parser')
          files = sorted([a['href'] for a in soup_file.find_all('a') if \
                          a['href'].startswith('pageviews') and \
                          a['href'].endswith('.gz')], 
                          reverse=True)
          latest_files = files[:{{ vars.data_age }}]

          # latest_files_info = [(f"{base_url}{latest_year}{latest_month}{file}", os.path.splitext(file)[0]) 
          #                       for file in latest_files]
          # getting the full path of the file and just the name of the file, which is going to be used to name the parquet
          latest_files = [{'url':f"{base_url}{latest_year}{latest_month}{file}", 
                          'file_name': os.path.splitext(file)[0]}
                          for file in latest_files]
          # latest_urls, latest_files_no_ext = zip(*latest_files_info) if latest_files_info else ([], [])

          # print(json.dumps({"latest_pageviews_urls": latest_urls}))

          return json.dumps(latest_files)
          #return latest_files

      latest_urls= get_latest_pageviews_url("{{ vars.base_url }}")
      Kestra.outputs({
        'latest_urls': latest_urls
      })

  - id: process_files
    type: io.kestra.plugin.core.flow.ForEach
    values: "{{ outputs.latest_pageviews.vars.latest_urls }}"
    tasks:
    - id: log        
      type: io.kestra.plugin.core.log.Log        
      message: "{{ json(taskrun.value) }}"

    - id: download_pageviews
      type: io.kestra.plugin.core.http.Download
      uri: "{{ json(taskrun.value).url }}"

    - id: log_download_output
      type: io.kestra.plugin.core.log.Log
      message: "{{ outputs }}"
      
    - id: pageviews_to_parquet
      type: io.kestra.plugin.scripts.python.Script
      outputFiles:
        - "pageviews_table.parquet"
      inputFiles:
        page_views_file.gz: "{{ outputs.download_pageviews[taskrun.value].uri }}"
      beforeCommands:
        - pip install pandas
        - pip install fastparquet
      script: |
        import pandas as pd
        import os
        import gzip
        from kestra import Kestra

        def parse_pageviews_data(file_path: str):
          with gzip.open(file_path, 'rt', encoding='utf-8') as f:
            data = []
            for row, line in enumerate(f, 1):
                try:
                    line_cols = line.split()
                    lang_project, page_title, views, data_bytes = line_cols
                    lang_project_split = lang_project.split('.')
                    language = lang_project_split[0]# if len(lang_project_split) > 1 else ''
                    if len(language) <= 1: #Means no language was specified
                        language = ''
                        project = lang_project
                    else:
                        project = '.'.join(lang_project_split[1:]) if len(lang_project_split) > 1 else ''
                    data.append((language, project, page_title, int(views), int(data_bytes)))
                except Exception as e:
                    print(f'Error at row: {row}')
                    print(e)
                    print(f'line: {line}')

          df = pd.DataFrame(data, columns=['language', 'project', 'page_title',
                                  'views', 'bytes'])
          df.to_parquet('pageviews_table.parquet')
        parse_pageviews_data('page_views_file.gz')
        
    - id: upload_to_gcs
      type: io.kestra.plugin.gcp.gcs.Upload
      from: "{{ outputs.pageviews_to_parquet[taskrun.value].outputFiles['pageviews_table.parquet'] }}"
      to: "gs://{{ vars.gcs_bucket_name }}/{{ json(taskrun.value).file_name }}.parquet"

  #   #Starting dbt flow
  # - id: trigger_dbt
  #   type: io.kestra.plugin.core.flow.Trigger
  #   flowId: dbt-transformation-flow
  #   wait: false

triggers:
  - id: Hourly
    type: io.kestra.plugin.core.trigger.Schedule
    disabled: true
    cron: 3 * * * *