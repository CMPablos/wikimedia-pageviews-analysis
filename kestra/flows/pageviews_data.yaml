id: pageviews-data
namespace: wikimedia-pageviews
description: Finds the latest pageviews URL

variables:
  base_url: "https://dumps.wikimedia.org/other/pageviews/"
  gcs_bucket_name: "wikimedia-pageviews-bucket"
  data_age: 24 # Files to be extracted, 1 file represents 1 hour of data
  #limiting the languages to reduce the size of the data.To enable all languages set one element of the list as 'all'
  select_languages: 
    -'en'
    -'es'
    -'de'
    -'fr'

tasks:
  - id: existing_pageviews_in_bucket
    type: io.kestra.plugin.gcp.gcs.List
    from: "gs://{{ vars.gcs_bucket_name }}/"

  - id: current_pageviews
    type: io.kestra.plugin.core.log.Log        
    message: "Found existing pageviews in bucket:\n {{ outputs.existing_pageviews_in_bucket.blobs }}"


  - id: latest_pageviews
    type: io.kestra.plugin.scripts.python.Script
    beforeCommands:
      - pip install requests beautifulsoup4
    script: |
      import requests
      from bs4 import BeautifulSoup
      import sys
      import json
      import os
      from kestra import Kestra


      def get_latest_pageviews_url(base_url: str, files_in_bucket: list[str] = []):
          resp = requests.get(base_url)
          soup = BeautifulSoup(resp.content, 'html.parser')

          # Get latest year
          years = sorted([a['href'] for a in soup.find_all('a') if a['href'].strip('/').isdigit()], reverse=True)
          latest_year = years[0]

          # Get latest month
          resp_month = requests.get(f'{base_url}{latest_year}')
          soup_month = BeautifulSoup(resp_month.content, 'html.parser')
          months = sorted([a['href'] for a in soup_month.find_all('a') if \
                          latest_year.strip('/') in a['href']], 
                          reverse=True)
          latest_month = months[0]

          # Get latest file
          resp_file = requests.get(f'{base_url}{latest_year}{latest_month}')
          soup_file = BeautifulSoup(resp_file.content, 'html.parser')
          files = sorted([a['href'] for a in soup_file.find_all('a') if \
                          a['href'].startswith('pageviews') and \
                          a['href'].endswith('.gz')], 
                          reverse=True)

          files_in_bucket = [os.path.splitext(file.split('/')[-1])[0] for file in files_in_bucket]

          latest_files = files[:{{ vars.data_age }}]
          print(f'Latest files: {latest_files}')
          latest_files = [file for file in latest_files if os.path.splitext(file)[0] not in files_in_bucket]
          print(f'Keeping: {latest_files}')

          latest_files = [{'url':f"{base_url}{latest_year}{latest_month}{file}", 
                          'file_name': os.path.splitext(file)[0]}
                          for file in latest_files]

          return json.dumps(latest_files)
          #return latest_files

      # Getting just the file names from the uri
      # Parse the GCS blob information
      blobs = json.loads("""{{ outputs.existing_pageviews_in_bucket.blobs | json }}""")
      
      # Extract just the file names from the blobs
      files_in_bucket = []
      for blob in blobs:
          # Get the filename from the 'name' field
          file_name = blob.get('name', '')
          if file_name:
              # Just the filename without path
              files_in_bucket.append(os.path.basename(file_name))
      
      print(f"Extracted file names from bucket: {files_in_bucket}")
      latest_urls= get_latest_pageviews_url(base_url = "{{ vars.base_url }}",
                                            files_in_bucket = files_in_bucket)

      if len(latest_urls) == 0:
        print('No new files found')
      Kestra.outputs({
        'latest_urls': latest_urls
      })

  - id: process_files
    type: io.kestra.plugin.core.flow.ForEach
    values: "{{ outputs.latest_pageviews.vars.latest_urls }}"
    tasks:
    - id: current_url   
      type: io.kestra.plugin.core.log.Log        
      message: "Processing current task:\n {{ json(taskrun.value) }}"

    - id: download_pageviews
      type: io.kestra.plugin.core.http.Download
      uri: "{{ json(taskrun.value).url }}"

    - id: log_download_output
      type: io.kestra.plugin.core.log.Log
      message: "{{ outputs }}"
      
    - id: pageviews_to_parquet
      type: io.kestra.plugin.scripts.python.Script
      outputFiles:
        - "pageviews_table.parquet"
      inputFiles:
        page_views_file.gz: "{{ outputs.download_pageviews[taskrun.value].uri }}"
      beforeCommands:
        - pip install pandas
        - pip install fastparquet
      script: |
        import pandas as pd
        import os
        import gzip
        from datetime import datetime
        from kestra import Kestra

        url = '{{ json(taskrun.value).url }}'
        def parse_pageviews_data(file_path: str, select_languages: list[str]):
          filename = os.path.splitext(os.path.split(url)[-1])[0]
          date_time_str = ' '.join(filename.split('-')[-2:])
          date_time = datetime.strptime(date_time_str, '%Y%m%d %H%M%S')
          with gzip.open(file_path, 'rt', encoding='utf-8') as f:
            data = []
            for row, line in enumerate(f, 1):
              try:
                line_cols = line.split()
                lang_project, page_title, views, data_bytes = line_cols
                lang_project_split = lang_project.split('.')
                language = lang_project_split[0]  # if len(lang_project_split) > 1 else None
                if ('all' not in select_languages) & (language not in select_languages):
                  continue
                if len(language) <= 1:
                    language = None
                    project = lang_project
                else:
                    project = '.'.join(lang_project_split[1:]) if len(lang_project_split) > 1 else None
                data.append((language, project, page_title, int(views), int(data_bytes), date_time, url))
              except Exception as e:
                print(f'Error at row: {row}')
                print(e)
                print(f'line: {line}')
                  
          df = pd.DataFrame(data, columns=['language', 'project', 'page_title',
                            'views', 'bytes', 'date', 'url'])
          df.to_parquet('pageviews_table.parquet')
        parse_pageviews_data('page_views_file.gz', select_languages = "{{ vars.select_languages }}")
        
    - id: upload_to_gcs
      type: io.kestra.plugin.gcp.gcs.Upload
      from: "{{ outputs.pageviews_to_parquet[taskrun.value].outputFiles['pageviews_table.parquet'] }}"
      to: "gs://{{ vars.gcs_bucket_name }}/{{ json(taskrun.value).file_name }}.parquet"

triggers:
  - id: Hourly
    type: io.kestra.plugin.core.trigger.Schedule
    disabled: true
    cron: 3 * * * *